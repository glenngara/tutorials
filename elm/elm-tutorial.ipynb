{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Learning Machines from Scratch\n",
    "### Author: [Glenn Paul Gara](glenngara.github.io)\n",
    "<!--The notebook is originally uploaded to the [GitHub repository](github.com).-->\n",
    "\n",
    "<!---\n",
    "Extreme Learning Machines (ELMs) are single-hidden layer feedforward neural networks (SLFN) that learns very fast compared to gradient-based learning techniques. Itâ€™s like a classical one hidden layer neural network without a learning process. This kind of neural network does not perform iterative tuning, making it faster than networks trained using backpropagation method with better generalization performance as claimed by the author.\n",
    "\n",
    "ELM is based on the Universal Approximation Theorem which states that:\n",
    "\n",
    "> _\"A feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of Rn, under mild assumptions on the activation function.\"_\n",
    "\n",
    "This means that ELM can solve a classification and regression tasks with significant accuracy if it has sufficient hidden neurons and training data to learn for all hidden neurons.\n",
    "\n",
    "In this tutorial, we will build an Extreme Learning Machine model for `MNIST handwritten digits`.\n",
    "\n",
    "### Outline\n",
    "\n",
    "1. Package Imports\n",
    "2. Dataset Loading and Preprocessing\n",
    "3. Network Initialization\n",
    "-->\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import necessary packages for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.linalg import pinv2\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to load our datasets to train our ELM and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/mnist_train.csv')\n",
    "test = pd.read_csv('dataset/mnist_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a `MinMaxScaler` to normalize our features within the range of (0,1), and a `OneHotEncoder` to transform our targets into one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotencoder = OneHotEncoder(categories='auto')\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(train.values[:,1:])\n",
    "y_train = onehotencoder.fit_transform(train.values[:,:1]).toarray()\n",
    "\n",
    "X_test = scaler.fit_transform(test.values[:,1:])\n",
    "y_test = onehotencoder.fit_transform(test.values[:,:1]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialize our network, we need to identify the following:\n",
    "1. The size of the input layer, which is the number of input features\n",
    "2. Number of hidden neurons\n",
    "3. Input to hidden weights\n",
    "4. Hidden layer activation function\n",
    "\n",
    "The size of the input layer refers to the number of input features of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initalize the number of hidden neurons to 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to initialize input weights and biases randomly drawn from a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_weights = np.random.normal(size=[input_size,hidden_size])\n",
    "biases = np.random.normal(size=[hidden_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden layer activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_nodes(X):\n",
    "    G = safe_sparse_dot(X, input_weights)\n",
    "    G = G + biases\n",
    "    H = relu(G)\n",
    "    return H\n",
    "\n",
    "output_weights = np.dot(pinv2(hidden_nodes(X_train)), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    out = hidden_nodes(X)\n",
    "    out = np.dot(out, output_weights)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for  1000  hidden nodes:  0.9427\n"
     ]
    }
   ],
   "source": [
    "prediction = predict(X_test)\n",
    "correct = 0\n",
    "total = X_test.shape[0]\n",
    "\n",
    "for i in range(total):\n",
    "    predicted = np.argmax(prediction[i])\n",
    "    actual = np.argmax(y_test[i])\n",
    "    correct += 1 if predicted == actual else 0\n",
    "accuracy = correct/total\n",
    "print('Accuracy for ', hidden_size, ' hidden nodes: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
